{"name":"Rocketjob","tagline":"High volume, priority based, background job processing solution for Ruby.","body":"# rocketjob[![Build Status](https://secure.travis-ci.org/rocketjob/rocketjob.png?branch=master)](http://travis-ci.org/rocketjob/rocketjob) ![](http://ruby-gem-downloads-badge.herokuapp.com/rocketjob?type=total)\r\n\r\nHigh volume, priority based, background job processing solution for Ruby.\r\n\r\n## Status\r\n\r\nBeta - Feedback on the API is welcome. API may change.\r\n\r\nAlready in use in production internally processing large files with millions\r\nof records, as well as large jobs to walk though large databases.\r\n\r\n## Why?\r\n\r\nWe have tried for years to make both `resque` and more recently `sidekiq`\r\nwork for large high performance batch processing.\r\nEven `sidekiq-pro` was purchased and used in an attempt to process large batches.\r\n\r\nUnfortunately, after all the pain and suffering with the existing asynchronous\r\nworker solutions none of them have worked in our production environment without\r\nsignificant hand-holding and constant support. Mysteriously the odd record/job\r\nwas disappearing when processing 100's of millions of jobs with no indication\r\nwhere those lost jobs went.\r\n\r\nIn our environment we cannot lose even a single job or record, as all data is\r\nbusiness critical. The existing batch processing solution do not supply any way\r\nto collect the output from batch processing and as a result every job has custom\r\ncode to collect it's output. rocketjob has built in support to collect the results\r\nof any batch job.\r\n\r\nHigh availability and high throughput were being limited by how much we could get\r\nthrough `redis`. Being a single-threaded process it is constrained to a single\r\nCPU. Putting `redis` on a large multi-core box does not help since it will not\r\nuse more than one CPU at a time.\r\nAdditionally, `redis` is constrained to the amount of physical memory is available\r\non the server.\r\n`redis` worked very well when processing was below around 100,000 jobs a day,\r\nwhen our workload suddenly increased to over 100,000,000 a day it could not keep\r\nup. Its single CPU would often hit 100% CPU utilization when running many `sidekiq-pro`\r\nservers. We also had to store actual job data in a separate MySQL database since\r\nit would not fit in memory on the `redis` server.\r\n\r\n`rocketjob` was created out of necessity due to constant support. End-users were\r\nconstantly contacting the development team to ask on the status of \"hung\" or\r\n\"in-complete\" jobs, as part of our DevOps role.\r\n\r\nAnother significant production support challenge is trying to get `resque` or `sidekiq`\r\nto process the batch jobs in a very specific order. Switching from queue-based\r\nto priority-based job processing means that all jobs are processed in the order of\r\ntheir priority and not what queues are defined on what servers and in what quantity.\r\nThis approach has allowed us to significantly increase the CPU and IO utilization\r\nacross all worker machines. The traditional queue based approach required constant\r\ntweaking in the production environment to try and balance workload without overwhelming\r\nany one server.\r\n\r\nEnd-users are now able to modify the priority of their various jobs at runtime\r\nso that they can get that business critical job out first, instead of having to\r\nwait for other jobs of the same type/priority to finish first.\r\n\r\nSince `rocketjob` uploads the entire file, or all data for processing it does not\r\nrequire jobs to store the data in other databases.\r\nAdditionally, `rocketjob` supports encryption and compression of any data uploaded\r\ninto Sliced Jobs to ensure PCI compliance and to prevent sensitive from being exposed\r\neither at rest in the data store, or in flight as it is being read or written to the\r\nbackend data store.\r\nOften large files received for processing contain sensitive data that must not be exposed\r\nin the backend job store. Having this capability built-in ensures all our jobs\r\nare properly securing sensitive data.\r\n\r\nSince moving to `rocketjob` our production support has diminished and now we can\r\nfocus on writing code again. :)\r\n\r\n## Introduction\r\n\r\n`rocketjob` is a global \"priority based queue\" (https://en.wikipedia.org/wiki/Priority_queue)\r\nAll jobs are placed in a single global queue and the job with the highest priority\r\nis processed first. Jobs with the same priority are processed on a first-in\r\nfirst-out (FIFO) basis.\r\n\r\nThis differs from the traditional approach of separate queues for jobs which\r\nquickly becomes cumbersome when there are for example over a hundred different\r\ntypes of jobs.\r\n\r\nThe global priority based queue ensures that the workers are utilized to their\r\ncapacity without requiring constant manual intervention.\r\n\r\n`rocketjob` is designed to handle hundreds of millions of concurrent jobs\r\nthat are often encountered in high volume batch processing environments.\r\nIt is designed from the ground up to support large batch file processing.\r\nFor example a single file that contains millions of records to be processed\r\nas quickly as possible without impacting other jobs with a higher priority.\r\n\r\n## Management\r\n\r\nThe companion project [rocketjob mission control](https://github.com/rocketjob/rocket_job_mission_control)\r\ncontains the Rails Engine that can be loaded into your Rails project to add\r\na web interface for viewing and managing `rocketjob` jobs.\r\n\r\n`rocketjob mission control` can also be run stand-alone in a shell Rails application.\r\n\r\nBy separating `rocketjob mission control` into a separate gem means it does not\r\nhave to be loaded where `rocketjob` jobs are defined or run.\r\n\r\n## Jobs\r\n\r\nSimple single task jobs:\r\n\r\nExample job to run in a separate worker process\r\n\r\n```ruby\r\nclass MyJob < RocketJob::Job\r\n  # Method to call asynchronously by the worker\r\n  def perform(email_address, message)\r\n    # For example send an email to the supplied address with the supplied message\r\n    send_email(email_address, message)\r\n  end\r\nend\r\n```\r\n\r\nTo queue the above job for processing:\r\n\r\n```ruby\r\nMyJob.perform_later('jack@blah.com', 'lets meet')\r\n```\r\n\r\n## Directory Monitoring\r\n\r\nA common task with many batch processing systems is to look for the appearance of\r\nnew files and kick off jobs to process them. `DirmonJob` is a job designed to do\r\nthis task.\r\n\r\n`DirmonJob` runs every 5 minutes by default, looking for new files that have appeared\r\nbased on configured entries called `DirmonEntry`. Ultimately these entries will be\r\nconfigurable via `rocketjob_mission_control`, the web management interface for `rocketjob`.\r\n\r\nExample, creating a `DirmonEntry`\r\n\r\n```ruby\r\nRocketJob::DirmonEntry.new(\r\n  path:         'path_to_monitor/*',\r\n  job:          'Jobs::TestJob',\r\n  arguments:    [ { input: 'yes' } ],\r\n  properties:   { priority: 23, perform_method: :event },\r\n  archive_directory: '/exports/archive'\r\n)\r\n```\r\n\r\nThe attributes of DirmonEntry:\r\n\r\n* path <String>\r\n\r\nWildcard path to search for files in.\r\nFor details on valid path values, see: http://ruby-doc.org/core-2.2.2/Dir.html#method-c-glob\r\n\r\nExample:\r\n\r\n    * input_files/process1/*.csv*\r\n    * input_files/process2/**/*\r\n\r\n* job <String>\r\n\r\nName of the job to start\r\n\r\n* arguments <Array>\r\n\r\nAny user supplied arguments for the method invocation\r\nAll keys must be UTF-8 strings. The values can be any valid BSON type:\r\n\r\n    * Integer\r\n    * Float\r\n    * Time    (UTC)\r\n    * String  (UTF-8)\r\n    * Array\r\n    * Hash\r\n    * True\r\n    * False\r\n    * Symbol\r\n    * nil\r\n    * Regular Expression\r\n\r\n_Note_: Date is not supported, convert it to a UTC time\r\n\r\n* properties <Hash>\r\n\r\nAny job properties to set.\r\n\r\nExample, override the default job priority:\r\n\r\n```ruby\r\n{ priority: 45 }\r\n```\r\n\r\n* archive_directory\r\n\r\nArchive directory to move the file to before the job is started. It is important to\r\nmove the file before it is processed so that it is not picked up again for processing.\r\nIf no archive_directory is supplied the file will be moved to a folder called '_archive'\r\nin the same folder as the file itself.\r\n\r\nIf the `path` above is a relative path the relative path structure will be\r\nmaintained when the file is moved to the archive path.\r\n\r\n* enabled <Boolean>\r\n\r\nAllow a monitoring entry to be disabled so that it is ignored by `DirmonJob`.\r\nThis feature is useful for operations to temporarily stop processing files\r\nfrom a particular source, without having to completely delete the `DirmonEntry`.\r\nIt can also be used to create a `DirmonEntry` without it becoming immediately\r\nactive.\r\n```\r\n\r\n### Starting the directory monitor\r\n\r\nThe directory monitor job only needs to be started once per installation by running\r\nthe following code:\r\n\r\n```ruby\r\nRocketJob::Jobs::DirmonJob.perform_later\r\n```\r\n\r\nThe polling interval to check for new files can be modified when starting the job\r\nfor the first time by adding:\r\n```ruby\r\nRocketJob::Jobs::DirmonJob.perform_later do |job|\r\n  job.check_seconds = 180\r\nend\r\n```\r\n\r\nThe default priority for `DirmonJob` is 40, to increase it's priority:\r\n```ruby\r\nRocketJob::Jobs::DirmonJob.perform_later do |job|\r\n  job.check_seconds = 300\r\n  job.priority      = 25\r\nend\r\n```\r\n\r\nOnce `DirmonJob` has been started it's priority and check interval can be\r\nchanged at any time as follows:\r\n\r\n```ruby\r\nRocketJob::Jobs::DirmonJob.first.set(check_seconds: 180, priority: 20)\r\n```\r\n\r\nThe `DirmonJob` will automatically re-schedule a new instance of itself to run in\r\nthe future after it completes a each scan/run. If successful the current job instance\r\nwill destroy itself.\r\n\r\nIn this way it avoids having a single Directory Monitor process that constantly\r\nsits there monitoring folders for changes. More importantly it avoids a \"single\r\npoint of failure\" that is typical for earlier directory monitoring solutions.\r\nEvery time `DirmonJob` runs and scans the paths for new files it could be running\r\non a new worker. If any server/worker is removed or shutdown it will not stop\r\n`DirmonJob` since it will just run on another worker instance.\r\n\r\nThere can only be one `DirmonJob` instance `queued` or `running` at a time. Any\r\nattempt to start a second instance will result in an exception.\r\n\r\nIf an exception occurs while running `DirmonJob`, a failed job instance will remain\r\nin the job list for problem determination. The failed job cannot be restarted and\r\nshould be destroyed if no longer needed.\r\n\r\n## Rails Configuration\r\n\r\nMongoMapper will already configure itself in Rails environments. `rocketjob` can\r\nbe configured to use a separate MongoDB instance from the Rails application as follows:\r\n\r\nFor example, we may want `RocketJob::Job` to be stored in a Mongo Database that\r\nis replicated across data centers, whereas we may not want to replicate the\r\n`RocketJob::SlicedJob`** slices due to it's sheer volume.\r\n\r\n```ruby\r\nconfig.before_initialize do\r\n  # Share the common mongo configuration file\r\n  config_file = root.join('config', 'mongo.yml')\r\n  if config_file.file?\r\n    config = YAML.load(ERB.new(config_file.read).result)\r\n    if config[\"#{Rails.env}_rocketjob]\r\n      options = (config['options']||{}).symbolize_keys\r\n      options[:logger] = SemanticLogger::DebugAsTraceLogger.new('Mongo:rocketjob')\r\n      RocketJob::Config.mongo_connection = Mongo::MongoClient.from_uri(config['uri'], options)\r\n    end\r\n    # It is also possible to store the jobs themselves in a separate MongoDB database\r\n    if config[\"#{Rails.env}_rocketjob_work]\r\n      options = (config['options']||{}).symbolize_keys\r\n      options[:logger] = SemanticLogger::DebugAsTraceLogger.new('Mongo:rocketjob_work')\r\n      RocketJob::Config.mongo_work_connection = Mongo::MongoClient.from_uri(config['uri'], options)\r\n    end\r\n  else\r\n    puts \"\\nmongo.yml config file not found: #{config_file}\"\r\n  end\r\nend\r\n```\r\n\r\nFor an example config file, `config/mongo.yml`, see [mongo.yml](https://github.com/rocketjob/rocketjob/blob/master/test/config/mongo.yml)\r\n\r\n## Standalone Configuration\r\n\r\nWhen running `rocketjob` in a standalone environment without Rails, the MongoDB\r\nconnections will need to be setup as follows:\r\n\r\n```ruby\r\noptions = {\r\n  pool_size:    50,\r\n  pool_timeout: 5,\r\n  logger:       SemanticLogger::DebugAsTraceLogger.new('Mongo:Work'),\r\n}\r\n\r\n# For example when using a replica-set for high availability\r\nuri = 'mongodb://mongo1.site.com:27017,mongo2.site.com:27017/production_rocketjob'\r\nRocketJob::Config.mongo_connection = Mongo::MongoClient.from_uri(uri, options)\r\n\r\n# Use a separate database, or even server for `RocketJob::SlicedJob` slices\r\nuri = 'mongodb://mongo1.site.com:27017,mongo2.site.com:27017/production_rocketjob_slices'\r\nRocketJob::Config.mongo_work_connection = Mongo::MongoClient.from_uri(uri, options)\r\n```\r\n\r\n## Requirements\r\n\r\nMongoDB V2.6 or greater. V3 is recommended\r\n\r\n* V2.6 includes a feature to allow lookups using the `$or` clause to use an index\r\n\r\n## Meta\r\n\r\n* Code: `git clone git://github.com/rocketjob/rocketjob.git`\r\n* Home: <https://github.com/rocketjob/rocketjob>\r\n* Bugs: <http://github.com/rocketjob/rocketjob/issues>\r\n* Gems: <http://rubygems.org/gems/rocketjob>\r\n\r\nThis project uses [Semantic Versioning](http://semver.org/).\r\n\r\n## Author\r\n\r\n[Reid Morrison](https://github.com/reidmorrison) :: @reidmorrison\r\n\r\n## Contributors\r\n\r\n* [Chris Lamb](https://github.com/lambcr)\r\n","google":"UA-52339082-4","note":"Don't delete this file! It's used internally to help with page regeneration."}